{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEoybFElAON3"
      },
      "source": [
        "# Agile Modeling for Bioacoustics.\n",
        "\n",
        "This notebook provides a workflow for creating custom classifiers for target signals, by first **searching** for training data, and then engaging in an **active learning** loop.\n",
        "\n",
        "We assume that embeddings have been pre-computed using `embed.ipynb`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gntw2Wq9Atpp"
      },
      "source": [
        "## Configuration and Imports.\n",
        "\n",
        "Run in the perch_conda_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "58HLTIdcAzte"
      },
      "outputs": [],
      "source": [
        " #@title Imports. { vertical-output: true }\n",
        "\n",
        "import collections\n",
        "from etils import epath\n",
        "from ml_collections import config_dict\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "from chirp.inference import colab_utils\n",
        "colab_utils.initialize(use_tf_gpu=True, disable_warnings=True)\n",
        "\n",
        "from chirp import audio_utils\n",
        "#from chirp.inference import a2o_utils\n",
        "from chirp.inference import interface\n",
        "from chirp.inference import tf_examples\n",
        "from chirp.inference import models\n",
        "from chirp.models import metrics\n",
        "from chirp.taxonomy import namespace\n",
        "from chirp.inference.search import bootstrap\n",
        "from chirp.inference.search import search\n",
        "from chirp.inference.search import display\n",
        "from chirp.inference.classify import classify\n",
        "from chirp.inference.classify import data_lib\n",
        "\n",
        "# New\n",
        "from chirp.inference import embed_lib\n",
        "from IPython.display import display as ipy_display, HTML\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available: 1\n"
          ]
        }
      ],
      "source": [
        "# Check tf can access the GPU\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3MDx1FKzSxuS"
      },
      "outputs": [],
      "source": [
        "#@title Set Paths\n",
        "base_dir = os.getenv('BASE_DIR')\n",
        "if not base_dir:\n",
        "    raise ValueError(\"BASE_DIR environment variable is not set.\")\n",
        "\n",
        "country = 'indonesia'\n",
        "project_directory = os.path.join(base_dir, 'marrs_acoustics/data/output_dir_' + country)\n",
        "embeddings_path = os.path.join(project_directory + '/raw_embeddings/')\n",
        "#embeddings_path = '/content/gs/soundscapes/benwilliams_reef/embeddings/' ### new gc embeddings\n",
        "all_target_sounds = os.path.join(project_directory + '/target_sounds/')\n",
        "\n",
        "# Configs:\n",
        "sample_rate = 32000\n",
        "window_size = 5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/bwilliams/ucl_projects/marrs_acoustics/data/output_dirindonesia'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "project_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_fMOBVK9A_O1"
      },
      "outputs": [],
      "source": [
        "#@title Basic Configuration. { vertical-output: true }\n",
        "\n",
        "#@markdown Choose what data to work with.\n",
        "#@markdown * For local data (most cases), choose 'filesystem'.\n",
        "#@markdown * For Australian Acoustic Observatory, select 'a2o'.\n",
        "#@markdown This will cause many options (like model_choice) to be overridden.\n",
        "#@markdown Note that you will need an Authentication Token from:\n",
        "#@markdown https://data.acousticobservatory.org/my_account\n",
        "data_source = 'filesystem' #@param['filesystem', 'a2o']\n",
        "a2o_auth_token = '' #@param {type:'string'}\n",
        "\n",
        "#@markdown Define the model: Usually perch or birdnet.\n",
        "model_choice = 'surfperch'  #@param {type:'string'}\n",
        "#@markdown Set the base directory for the project.\n",
        "working_dir = '/tmp/agile'  #@param {type:'string'}\n",
        "\n",
        "#@markdown Set the embedding and labeled data directories.\n",
        "#labeled_data_path = labeled_data_path ###epath.Path(working_dir) / 'labeled'\n",
        "#custom_classifier_path = custom_classifier_path ###epath.Path(working_dir) / 'custom_classifier'\n",
        "\n",
        "#@markdown The embeddings_path should be detected automatically, but can be\n",
        "#@markdown overridden.\n",
        "embeddings_path = embeddings_path\n",
        "\n",
        "#@markdown OPTIONAL: Set up separation model.\n",
        "separation_model_key = ''  #@param {type:'string'}\n",
        "separation_model_path = ''  #@param {type:'string'}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HLJxpglygs9X"
      },
      "outputs": [],
      "source": [
        "#@title SurfPerch Configuration. { vertical-output: true }\n",
        "\n",
        "config = config_dict.ConfigDict()\n",
        "config.embed_fn_config = config_dict.ConfigDict()\n",
        "config.embed_fn_config.model_config = config_dict.ConfigDict()\n",
        "\n",
        "config.source_file_patterns = ['/media/mars_5tb_drive/mars_global_acoustic_study/indonesia_acoustics/raw_audio/*.[wW][aA][vV]']  #@param\n",
        "config.output_dir = embeddings_path\n",
        "\n",
        "#@markdown For Perch, set the perch_tfhub_model_version, and the model will load\n",
        "#@markdown automagically from TFHub. Alternatively, set the model path for a\n",
        "#@markdown local copy of the model.\n",
        "#@markdown Note that only one of perch_model_path and perch_tfhub_version should\n",
        "#@markdown be set.\n",
        "#perch_tfhub_version = 8  #@param\n",
        "surfperch_path = os.path.join(base_dir, 'marrs_acoustics/SurfPerch-model')\n",
        "\n",
        "#@markdown For BirdNET, point to the specific tflite file.\n",
        "config.embed_fn_config.model_key = 'taxonomy_model_tf'\n",
        "config.embed_fn_config.model_config.window_size_s = window_size\n",
        "config.embed_fn_config.model_config.hop_size_s = 5.0\n",
        "config.embed_fn_config.model_config.sample_rate = sample_rate\n",
        "#config.embed_fn_config.model_config.tfhub_version = perch_tfhub_version\n",
        "config.embed_fn_config.model_config.model_path = surfperch_path\n",
        "\n",
        "# Only write embeddings to reduce size.\n",
        "config.embed_fn_config.write_embeddings = True\n",
        "config.embed_fn_config.write_logits = False\n",
        "config.embed_fn_config.write_separated_audio = False\n",
        "config.embed_fn_config.write_raw_audio = False\n",
        "\n",
        "# Number of parent directories to include in the filename.\n",
        "config.embed_fn_config.file_id_depth = 1\n",
        "\n",
        "# Uncomment to check the model path and fpr reef classes (e.g antrop_bomb) to be sure we've got SurfPerch\n",
        "#embed_fn.embedding_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bZRQbL3ChJoS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Loading model(s)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-05 18:51:11.833390: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-11-05 18:51:11.833667: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-11-05 18:51:11.833806: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-11-05 18:51:11.916479: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-11-05 18:51:11.916743: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-11-05 18:51:11.916900: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-11-05 18:51:11.917015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6302 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 267776 source infos.\n",
            "\n",
            "\n",
            "Test-run of model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1730832680.522848   10318 service.cc:145] XLA service 0x15cdb800 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1730832680.522894   10318 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
            "2024-11-05 18:51:20.887579: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "W0000 00:00:1730832680.925036   10318 assert_op.cc:38] Ignoring Assert operator jax2tf_infer_fn_/assert_equal_1/Assert/AssertGuard/Assert\n",
            "2024-11-05 18:51:21.667835: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
            "2024-11-05 18:51:22.130440: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:742] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1730832683.375062   12549 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot', 300 bytes spill stores, 256 bytes spill loads\n",
            "\n",
            "I0000 00:00:1730832683.466267   12554 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot', 324 bytes spill stores, 316 bytes spill loads\n",
            "\n",
            "I0000 00:00:1730832683.616369   12556 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot', 240 bytes spill stores, 188 bytes spill loads\n",
            "\n",
            "I0000 00:00:1730832683.839600   12564 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot', 1188 bytes spill stores, 1180 bytes spill loads\n",
            "\n",
            "I0000 00:00:1730832683.938358   12560 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot', 740 bytes spill stores, 720 bytes spill loads\n",
            "\n",
            "I0000 00:00:1730832684.093737   12558 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot', 1392 bytes spill stores, 1392 bytes spill loads\n",
            "\n",
            "I0000 00:00:1730832684.221647   12545 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot', 164 bytes spill stores, 172 bytes spill loads\n",
            "\n",
            "I0000 00:00:1730832684.257073   12561 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot', 48 bytes spill stores, 48 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1730832690.216880   10318 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        }
      ],
      "source": [
        "#@title SurfPerch test run. { vertical-output: true }\n",
        "\n",
        "# Set up the embedding function, including loading models.\n",
        "embed_fn = embed_lib.EmbedFn(**config.embed_fn_config)\n",
        "print('\\n\\nLoading model(s)...')\n",
        "embed_fn.setup()\n",
        "\n",
        "# Create output directory and write the configuration.\n",
        "output_dir = epath.Path(config.output_dir)\n",
        "output_dir.mkdir(exist_ok=True, parents=True)\n",
        "#embed_lib.maybe_write_config(config, output_dir) ### comment out if accessing frm=om gc\n",
        "\n",
        "# Create SourceInfos.\n",
        "source_infos = embed_lib.create_source_infos(\n",
        "    config.source_file_patterns,\n",
        "    num_shards_per_file=config.get('num_shards_per_file', -1),\n",
        "    shard_len_s=config.get('shard_len_s', -1))\n",
        "print(f'Found {len(source_infos)} source infos.')\n",
        "\n",
        "print('\\n\\nTest-run of model...')\n",
        "window_size_s = config.embed_fn_config.model_config.window_size_s\n",
        "sr = config.embed_fn_config.model_config.sample_rate\n",
        "z = np.zeros([int(sr * window_size_s)])\n",
        "embed_fn.embedding_model.embed(z)\n",
        "print('Setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSCqxovpVibG"
      },
      "source": [
        "## Select target sound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Nwqpd9BWVlI_"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/home/bwilliams/ucl_projects/marrs_acoustics/data/Indonesia/target_sounds/'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title { vertical-output: true }\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Path to cioc target sound folders\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ls_target_sounds \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_target_sounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# For each target sound folder, find the first audio file as an example\u001b[39;00m\n\u001b[1;32m      6\u001b[0m example_target_sounds \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/bwilliams/ucl_projects/marrs_acoustics/data/Indonesia/target_sounds/'"
          ]
        }
      ],
      "source": [
        "#@title { vertical-output: true }\n",
        "# Path to cioc target sound folders\n",
        "ls_target_sounds = os.listdir(all_target_sounds)\n",
        "\n",
        "# For each target sound folder, find the first audio file as an example\n",
        "example_target_sounds = []\n",
        "for folder in ls_target_sounds:\n",
        "  wav_files = [file for file in os.listdir(os.path.join(all_target_sounds, folder)) if file.lower().endswith('.wav')]\n",
        "  example_sound_path = os.path.join(all_target_sounds, folder + '/' + wav_files[0])\n",
        "  example_target_sounds.append(example_sound_path)\n",
        "\n",
        "# Now view each example target sound\n",
        "print('Number of different target sounds: ', len(example_target_sounds))\n",
        "for audio_path in example_target_sounds:\n",
        "  print('Target sound label: ', audio_path.split('/')[-2])\n",
        "  audio = audio_utils.load_audio(audio_path, target_sample_rate = sample_rate)\n",
        "  display.plot_audio_melspec(audio, sample_rate = sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WomRlHFGW47p"
      },
      "outputs": [],
      "source": [
        "# @title Hit run on this cell and pick a target sound\n",
        "\n",
        "# Ensure the path exists and list directories\n",
        "if os.path.exists(all_target_sounds):\n",
        "    sound_folders = [f for f in os.listdir(all_target_sounds) if os.path.isdir(os.path.join(all_target_sounds, f))]\n",
        "else:\n",
        "    print(\"Path does not exist:\", all_target_sounds)\n",
        "    sound_folders = []\n",
        "\n",
        "# Create and display the dropdown\n",
        "sound_dropdown = widgets.Dropdown(\n",
        "    options=sound_folders,\n",
        "    description='Select sound:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "# Define a function that reacts to changes in the dropdown\n",
        "def on_sound_change(change):\n",
        "    choice = change['new']\n",
        "    print(f'Changed target sound to: {choice}')\n",
        "\n",
        "# Attach the observer to the dropdown\n",
        "sound_dropdown.observe(on_sound_change, names='value')\n",
        "\n",
        "ipy_display(sound_dropdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RjFJ-9yVuut"
      },
      "outputs": [],
      "source": [
        "#@title Set additional paths for newly selected target sound\n",
        "target_sound = sound_dropdown.value\n",
        "\n",
        "labeled_data_path = os.path.join(project_directory + '/target_sounds/', target_sound)\n",
        "outputs_path = os.path.join(project_directory + '/outputs/')\n",
        "annotated_data_path = os.path.join(outputs_path, target_sound + '/labeled_outputs')\n",
        "custom_classifier_path = os.path.join(outputs_path, target_sound + '/custom_classifier')\n",
        "csv_output_filepath = os.path.join(outputs_path, target_sound + f'/{target_sound}_inference.csv')\n",
        "validation_log_filepath = (os.path.join(outputs_path, target_sound + f'/validation_{target_sound}.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix7GQPQvtqKZ"
      },
      "outputs": [],
      "source": [
        "#@title DELETE LATER { vertical-output: true }\n",
        "# pulse_train_path = '/content/drive/MyDrive/Chap3 MARRS Acoustics/data/Indonesia/outputs/pulse_train/labeled_outputs/pulse_train/'\n",
        "# # Select the target sound from the dropdown\n",
        "# target_sound = 'pulse_train'\n",
        "\n",
        "# # Retrieve all .wav files for the selected target sound\n",
        "# wav_files = [file for file in os.listdir(pulse_train_path) if file.lower().endswith('.wav')]\n",
        "\n",
        "# # Print the total number of audio files\n",
        "# print(f\"Number of indexed audio files in target sound directory: {len(wav_files)}\")\n",
        "\n",
        "# # Loop through all the .wav files and display their melspectrogram\n",
        "# for file_index, wav_file in enumerate(wav_files):\n",
        "#     audio_path = os.path.join(pulse_train_path, wav_file)\n",
        "#     print(f\"Viewing example file: {file_index + 1}\")\n",
        "#     audio = audio_utils.load_audio(audio_path, sample_rate)\n",
        "#     display.plot_audio_melspec(audio, sample_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAvOtOf5-_I8"
      },
      "outputs": [],
      "source": [
        "#@title Load and view examples of target sound { vertical-output: true }\n",
        "\n",
        "# Select the target sound from the dropdown\n",
        "target_sound = sound_dropdown.value\n",
        "\n",
        "# Retrieve all .wav files for the selected target sound\n",
        "wav_files = [file for file in os.listdir(labeled_data_path) if file.lower().endswith('.wav')]\n",
        "\n",
        "# Print the total number of audio files\n",
        "print(f\"Number of indexed audio files in target sound directory: {len(wav_files)}\")\n",
        "\n",
        "# Loop through all the .wav files and display their melspectrogram\n",
        "for file_index, wav_file in enumerate(wav_files):\n",
        "    audio_path = os.path.join(labeled_data_path, wav_file)\n",
        "    print(f\"Viewing example file: {file_index + 1}\")\n",
        "    audio = audio_utils.load_audio(audio_path, sample_rate)\n",
        "    display.plot_audio_melspec(audio, sample_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuLxgsgLc3FQ"
      },
      "outputs": [],
      "source": [
        "#@title Select audio clip { vertical-output: true }\n",
        "# Choose one of the files from above\n",
        "example_file = 2  #@param\n",
        "file_index = example_file -1\n",
        "\n",
        "# Set this file as the audio sample to move forward with\n",
        "selected_audio_path = os.path.join(labeled_data_path, wav_files[file_index])\n",
        "audio = audio_utils.load_audio(selected_audio_path, sample_rate)\n",
        "print(f\"Selected audio file: {selected_audio_path}\")\n",
        "\n",
        "# If you're audio clip is longer than 5s, adjust start_s to pick your\n",
        "# prefered start time.\n",
        "start_s = 0  #@param\n",
        "\n",
        "# Display the selected window.\n",
        "print('Selected audio window:')\n",
        "st = int(start_s * sample_rate)\n",
        "end = int(st + window_size * sample_rate)\n",
        "if end > audio.shape[0]:\n",
        "  end = audio.shape[0]\n",
        "  st = max([0, int(end - window_size * sample_rate)])\n",
        "audio_window = audio[st:end]\n",
        "display.plot_audio_melspec(audio_window, sample_rate)\n",
        "\n",
        "query_audio = audio_window\n",
        "sep_outputs = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c90l2ngEewdR"
      },
      "outputs": [],
      "source": [
        "#@title Copy existing labels to annotated data path\n",
        "positive_matches = annotated_data_path + '/' + target_sound\n",
        "if not os.path.exists(positive_matches):\n",
        "  os.makedirs(positive_matches)\n",
        "\n",
        "for file in os.listdir(labeled_data_path):\n",
        "    if file.lower().endswith('.wav'):\n",
        "        source_path = os.path.join(labeled_data_path, file)\n",
        "        destination_path = os.path.join(positive_matches, file)\n",
        "        shutil.copy2(source_path, destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c4gJ6S2ETKu"
      },
      "outputs": [],
      "source": [
        "#@title Load Project State and Models. { vertical-output: true }\n",
        "\n",
        "if data_source == 'a2o':\n",
        "  embedding_config = a2o_utils.get_a2o_embeddings_config()\n",
        "  bootstrap_config = bootstrap.BootstrapConfig.load_from_embedding_config(\n",
        "      embedding_config=embedding_config,\n",
        "      annotated_path=labeled_data_path,\n",
        "      embeddings_glob = '*/embeddings-*')\n",
        "  embeddings_path = embedding_config.output_dir\n",
        "elif (embeddings_path\n",
        "      or (epath.Path(working_dir) / 'embeddings/config.json').exists()):\n",
        "  if not embeddings_path:\n",
        "    # Use the default embeddings path, as it seems we found a config there.\n",
        "    embeddings_path = epath.Path(working_dir) / 'embeddings'\n",
        "  # Get relevant info from the embedding configuration.\n",
        "  bootstrap_config = bootstrap.BootstrapConfig.load_from_embedding_path(\n",
        "      embeddings_path=embeddings_path,\n",
        "      annotated_path=annotated_data_path) ###changed to prefered output dir\n",
        "  if (bootstrap_config.model_key == 'separate_embed_model'\n",
        "      and not separation_model_path.strip()):\n",
        "    separation_model_key = 'separator_model_tf'\n",
        "    separation_model_path = bootstrap_config.model_config.separator_model_tf_config.model_path\n",
        "else:\n",
        "  raise ValueError('No embedding configuration found.')\n",
        "\n",
        "project_state = bootstrap.BootstrapState(\n",
        "    bootstrap_config,\n",
        "    embedding_model=embed_fn.embedding_model, ###new\n",
        "    a2o_auth_token=a2o_auth_token,)\n",
        "\n",
        "# Load separation model.\n",
        "if separation_model_path:\n",
        "  separation_config = config_dict.ConfigDict({\n",
        "      'model_path': separation_model_path,\n",
        "      'frame_size': 32000,\n",
        "      'sample_rate': 32000,\n",
        "  })\n",
        "  separator = models.model_class_map()[\n",
        "      separation_model_key].from_config(separation_config)\n",
        "  print('Loaded separator model at {}'.format(separation_model_path))\n",
        "else:\n",
        "  print('No separation model loaded.')\n",
        "  separator = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atpdag0FGkud"
      },
      "outputs": [],
      "source": [
        "#@title Select the query channel. { vertical-output: true }\n",
        "\n",
        "#@markdown Choose a name for the class.\n",
        "query_label = target_sound\n",
        "#@markdown If you have applied separation, choose a channel.\n",
        "#@markdown Ignored if no separation model is being used.\n",
        "query_channel = 0  #@param\n",
        "\n",
        "if query_channel < 0 or sep_outputs is None:\n",
        "  query_audio = audio_window\n",
        "else:\n",
        "  query_audio = sep_outputs.separated_audio[query_channel].copy()\n",
        "\n",
        "display.plot_audio_melspec(query_audio, sample_rate)\n",
        "\n",
        "# Embed the audio sample with our model to generate the query vector\n",
        "outputs = project_state.embedding_model.embed(query_audio)\n",
        "query = outputs.pooled_embeddings('first', 'first')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0p0qkxcFSG0"
      },
      "source": [
        "## Search embeddings with query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOV_G29mGm_Z"
      },
      "outputs": [],
      "source": [
        "#@title Run Top-K Search. { vertical-output: true }\n",
        "\n",
        "#@markdown Number of search results to capture.\n",
        "top_k = 50  #@param\n",
        "\n",
        "#@markdown Target distance for search results.\n",
        "#@markdown This lets us try to hone in on a 'classifier boundary' instead of\n",
        "#@markdown just looking at the closest matches.\n",
        "#@markdown Set to 'None' for raw 'best results' search.\n",
        "target_score = None  #@param\n",
        "\n",
        "metric = 'euclidean'  #@param['euclidean', 'mip', 'cosine']\n",
        "\n",
        "random_sample = False  #@param\n",
        "\n",
        "ds = project_state.create_embeddings_dataset(shuffle_files=True)\n",
        "results, all_scores = search.search_embeddings_parallel(\n",
        "    ds, query,\n",
        "    hop_size_s=bootstrap_config.embedding_hop_size_s,\n",
        "    top_k=top_k, target_score=target_score, score_fn=metric,\n",
        "    random_sample=random_sample)\n",
        "\n",
        "# Plot histogram of distances\n",
        "ys, _, _ = plt.hist(all_scores, bins=128, density=True)\n",
        "hit_scores = [r.score for r in results.search_results]\n",
        "plt.scatter(hit_scores, np.zeros_like(hit_scores), marker='|',\n",
        "            color='r', alpha=0.5)\n",
        "\n",
        "plt.xlabel(metric)\n",
        "plt.ylabel('density')\n",
        "if target_score is not None:\n",
        "  plt.plot([target_score, target_score], [0.0, np.max(ys)], 'r:')\n",
        "  # Compute the proportion of scores < target_score\n",
        "  hit_percentage = (all_scores < target_score).mean()\n",
        "  print(f'score < target_score percentage : {hit_percentage:5.3f}')\n",
        "min_score = np.min(all_scores)\n",
        "plt.plot([min_score, min_score], [0.0, np.max(ys)], 'g:')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghaQtt5LbBJu"
      },
      "source": [
        "## **❗❗❗ START TIMING ❗❗❗**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8SYu-M2GsZx"
      },
      "outputs": [],
      "source": [
        "#@title Display results. { vertical-output: true }\n",
        "\n",
        "samples_per_page = 25\n",
        "page_state = display.PageState(\n",
        "    np.ceil(len(results.search_results) / samples_per_page))\n",
        "\n",
        "display.display_paged_results(\n",
        "    results, page_state, samples_per_page,\n",
        "    project_state=project_state,\n",
        "    embedding_sample_rate=project_state.embedding_model.sample_rate,\n",
        "    exclusive_labels=False,\n",
        "    checkbox_labels=[query_label, 'unknown'],\n",
        "    max_workers=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wi-TOGDGvh5"
      },
      "outputs": [],
      "source": [
        "#@title Write annotated examples. { vertical-output: true }\n",
        "results.write_labeled_data(bootstrap_config.annotated_path,\n",
        "                           project_state.embedding_model.sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbZkOXncFYKm"
      },
      "source": [
        "## Active Learning for a Target Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcLdypLeHmss"
      },
      "outputs": [],
      "source": [
        "# @title Load+Embed the Labeled Dataset. { vertical-output: true }\n",
        "\n",
        "#@markdown Time-pooling strategy for audio longer than the model's window size.\n",
        "time_pooling = 'mean'  # @param\n",
        "\n",
        "merged = data_lib.MergedDataset.from_folder_of_folders(\n",
        "    base_dir=annotated_data_path,\n",
        "    embedding_model=project_state.embedding_model,\n",
        "    time_pooling=time_pooling,\n",
        "    load_audio=False,\n",
        "    target_sample_rate=-2,\n",
        "    audio_file_pattern='*',\n",
        "    embedding_config_hash=bootstrap_config.embedding_config_hash(),\n",
        ")\n",
        "\n",
        "# Label distribution\n",
        "lbl_counts = np.sum(merged.data['label_hot'], axis=0)\n",
        "print('num classes :', (lbl_counts > 0).sum())\n",
        "print('mean ex / class :', lbl_counts.sum() / (lbl_counts > 0).sum())\n",
        "print('min ex / class :', (lbl_counts + (lbl_counts == 0) * 1e6).min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mY5rIiDHoE0"
      },
      "outputs": [],
      "source": [
        "#@title Train small model over embeddings. { vertical-output: true }\n",
        "\n",
        "#@markdown Number of random training examples to choose from each class.\n",
        "#@markdown Set exactly one of `train_ratio` and `train_examples_per_class`.\n",
        "train_ratio = 0.8  #@param\n",
        "train_examples_per_class = None  #@param\n",
        "\n",
        "#@markdown Number of random re-trainings. Allows judging model stability.\n",
        "num_seeds = 3  #@param\n",
        "\n",
        "# Classifier training hyperparams.\n",
        "# These should be good defaults.\n",
        "batch_size = 32\n",
        "num_epochs = 128\n",
        "num_hiddens = -1\n",
        "learning_rate = 1e-3\n",
        "\n",
        "metrics = collections.defaultdict(list)\n",
        "for seed in tqdm.tqdm(range(num_seeds)):\n",
        "  if num_hiddens > 0:\n",
        "    model = classify.get_two_layer_model(\n",
        "        num_hiddens, merged.embedding_dim, merged.num_classes)\n",
        "  else:\n",
        "    model = classify.get_linear_model(\n",
        "        merged.embedding_dim, merged.num_classes)\n",
        "  run_metrics = classify.train_embedding_model(\n",
        "      model, merged, train_ratio, train_examples_per_class,\n",
        "      num_epochs, seed, batch_size, learning_rate)\n",
        "  metrics['acc'].append(run_metrics.top1_accuracy)\n",
        "  metrics['auc_roc'].append(run_metrics.auc_roc)\n",
        "  metrics['cmap'].append(run_metrics.cmap_value)\n",
        "  metrics['maps'].append(run_metrics.class_maps)\n",
        "  metrics['test_logits'].append(run_metrics.test_logits)\n",
        "\n",
        "mean_acc = np.mean(metrics['acc'])\n",
        "mean_auc = np.mean(metrics['auc_roc'])\n",
        "mean_cmap = np.mean(metrics['cmap'])\n",
        "# Merge the test_logits into a single array.\n",
        "test_logits = {\n",
        "    k: np.concatenate([logits[k] for logits in metrics['test_logits']])\n",
        "    for k in metrics['test_logits'][0].keys()\n",
        "}\n",
        "\n",
        "print(f'acc:{mean_acc:5.2f}, auc_roc:{mean_auc:5.2f}, cmap:{mean_cmap:5.2f}')\n",
        "for lbl, auc in zip(merged.labels, run_metrics.class_maps):\n",
        "  if np.isnan(auc):\n",
        "    continue\n",
        "  print(f'\\n{lbl:8s}, auc_roc:{auc:5.2f}')\n",
        "  colab_utils.prstats(f'test_logits({lbl})',\n",
        "                      test_logits[merged.labels.index(lbl)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0Kg43YhH0p3"
      },
      "outputs": [],
      "source": [
        "#@title Run model on target unlabeled data. { vertical-output: true }\n",
        "\n",
        "#@markdown Choose the target class to work with.\n",
        "target_class = target_sound  #@param\n",
        "#@markdown Choose a target logit; will display results close to the target.\n",
        "#@markdown Set to None to get the highest-logit examples.\n",
        "target_logit = 0  #@param\n",
        "#@markdown Number of results to display.\n",
        "num_results = 200  #@param\n",
        "\n",
        "embeddings_ds = project_state.create_embeddings_dataset(\n",
        "    shuffle_files=True)\n",
        "target_class_idx = merged.labels.index(target_class)\n",
        "results, all_logits = search.classifer_search_embeddings_parallel(\n",
        "    embeddings_classifier=model,\n",
        "    target_index=target_class_idx,\n",
        "    embeddings_dataset=embeddings_ds,\n",
        "    hop_size_s=bootstrap_config.embedding_hop_size_s,\n",
        "    target_score=target_logit,\n",
        "    top_k=num_results\n",
        ")\n",
        "\n",
        "# Plot the histogram of logits.\n",
        "ys, _, _ = plt.hist(all_logits, bins=128, density=True)\n",
        "plt.xlabel(f'{target_class} logit')\n",
        "plt.ylabel('density')\n",
        "# plt.yscale('log')\n",
        "plt.plot([target_logit, target_logit], [0.0, np.max(ys)], 'r:')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHI2WJwPH2Wy"
      },
      "outputs": [],
      "source": [
        "#@title Display results for the target label. { vertical-output: true }\n",
        "\n",
        "display_labels = merged.labels\n",
        "\n",
        "#@markdown Specify any extra labels you would like displayed.\n",
        "extra_labels = []  #@param\n",
        "for label in extra_labels:\n",
        "  if label not in merged.labels:\n",
        "    display_labels += (label,)\n",
        "if 'unknown' not in merged.labels:\n",
        "  display_labels += ('unknown',)\n",
        "\n",
        "samples_per_page = 25\n",
        "page_state = display.PageState(\n",
        "    np.ceil(len(results.search_results) / samples_per_page))\n",
        "\n",
        "display.display_paged_results(\n",
        "    results, page_state, samples_per_page,\n",
        "    project_state=project_state,\n",
        "    embedding_sample_rate=project_state.embedding_model.sample_rate,\n",
        "    exclusive_labels=False,\n",
        "    checkbox_labels=display_labels,\n",
        "    max_workers=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDCcKfBGH4b_"
      },
      "outputs": [],
      "source": [
        "#@title Add selected results to the labeled data. { vertical-output: true }\n",
        "\n",
        "results.write_labeled_data(\n",
        "    bootstrap_config.annotated_path,\n",
        "    project_state.embedding_model.sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxasEcnhd7kP"
      },
      "outputs": [],
      "source": [
        "#@title Save the Custom Classifier. { vertical-output: true }\n",
        "\n",
        "wrapped_model = interface.LogitsOutputHead(\n",
        "    model_path=custom_classifier_path,\n",
        "    logits_key='logits',\n",
        "    logits_model=model,\n",
        "    class_list=namespace.ClassList('custom', merged.labels),\n",
        ")\n",
        "wrapped_model.save_model(\n",
        "    custom_classifier_path,\n",
        "    embeddings_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LbNiu3utyfR"
      },
      "outputs": [],
      "source": [
        "#@title Write classifier inference CSV. { vertical-output: true }\n",
        "\n",
        "#@markdown This cell writes detections (locations of audio windows where\n",
        "#@markdown the logit was greater than a threshold) to a CSV file.\n",
        "\n",
        "output_filepath = csv_output_filepath  #@param\n",
        "\n",
        "#@markdown Set the default detection thresholds, used for all classes.\n",
        "#@markdown To set per-class detection thresholds, modify the code below.\n",
        "#@markdown Keep in mind that thresholds are on the logit scale, so 0.0\n",
        "#@markdown corresponds to a 50% model confidence.\n",
        "default_threshold = 0.0  #@param\n",
        "if default_threshold is None:\n",
        "  # In this case, all logits are written. This can lead to very large CSV files.\n",
        "  class_thresholds = None\n",
        "else:\n",
        "  class_thresholds = collections.defaultdict(lambda: default_threshold)\n",
        "  # Add any per-class thresholds here.\n",
        "  class_thresholds[target_sound] = 1.0\n",
        "\n",
        "#@markdown Classes to ignore when counting detections.\n",
        "exclude_classes = ['unknown']  #@param\n",
        "\n",
        "#@markdown The `include_classes` list is ignored if empty.\n",
        "#@markdown If non-empty, only scores for these classes will be written.\n",
        "include_classes = []  #@param\n",
        "\n",
        "embeddings_ds = project_state.create_embeddings_dataset(\n",
        "    shuffle_files=True)\n",
        "classify.write_inference_csv(\n",
        "    embeddings_ds=embeddings_ds,\n",
        "    model=model,\n",
        "    labels=merged.labels,\n",
        "    output_filepath=output_filepath,\n",
        "    threshold=class_thresholds,\n",
        "    embedding_hop_size_s=bootstrap_config.embedding_hop_size_s,\n",
        "    include_classes=include_classes,\n",
        "    exclude_classes=exclude_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbl4Hz_Q7Uv5"
      },
      "source": [
        "# Call Density Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOGRdLjb7UH1"
      },
      "outputs": [],
      "source": [
        "cfg = config_dict.ConfigDict({\n",
        "    'model_path': custom_classifier_path,\n",
        "    'logits_key': 'custom',\n",
        "})\n",
        "logits_head = interface.LogitsOutputHead.from_config(cfg)\n",
        "model = logits_head.logits_model\n",
        "class_list = logits_head.class_list\n",
        "print('Loaded custom model with classes: ')\n",
        "print('\\t' + '\\n\\t'.join(class_list.classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uFSzYxk7ZoF"
      },
      "outputs": [],
      "source": [
        "#@title Validation and Call Density. { vertical-output: true }\n",
        "\n",
        "target_class = target_sound  #@param {type:'string'}\n",
        "\n",
        "#@markdown Bin bounds for validation. Should be an ordered list, beginning with\n",
        "#@markdown 0.0 and ending with 1.0.\n",
        "bounds = [0.0, 0.9, 0.99, 0.999, 1.0]  #@param\n",
        "#@markdown Number of validation samples per bin.\n",
        "samples_per_bin = 25  #@param\n",
        "\n",
        "bounds = np.array(bounds)\n",
        "num_bins = len(bounds) - 1\n",
        "\n",
        "# Select `top_k`` so that we are reasonably sure to get at least samples_per_bin\n",
        "# samples in the rarest bin in a randomly selected set of `top_k` examples.\n",
        "bin_probs = bounds[1:] - bounds[:-1]\n",
        "rarest_prob = np.min(bin_probs)\n",
        "top_k = samples_per_bin  / rarest_prob * 2\n",
        "\n",
        "embeddings_ds = project_state.create_embeddings_dataset(shuffle_files=True)\n",
        "results, all_logits = search.classifer_search_embeddings_parallel(\n",
        "    embeddings_classifier=logits_head,\n",
        "    target_index=class_list.classes.index(target_class),\n",
        "    random_sample=True,\n",
        "    top_k=top_k,\n",
        "    hop_size_s=bootstrap_config.embedding_hop_size_s,\n",
        "    embeddings_dataset=embeddings_ds,\n",
        ")\n",
        "\n",
        "q_bounds = np.quantile(all_logits, bounds)\n",
        "binned = [[] for _ in range(num_bins)]\n",
        "for r in results.search_results:\n",
        "  result_bin = np.argmax(r.score < q_bounds) - 1\n",
        "  binned[result_bin].append(r)\n",
        "binned = [np.random.choice(b, samples_per_bin, replace=False) for b in binned]\n",
        "\n",
        "combined_results = []\n",
        "for b in binned:\n",
        "  combined_results.extend(b)\n",
        "rng = np.random.default_rng(42)\n",
        "rng.shuffle(combined_results)\n",
        "\n",
        "ys, _, _, = plt.hist(all_logits, bins=100, density=True)\n",
        "for q in q_bounds:\n",
        "  plt.plot([q, q], [0.0, np.max(ys)], 'k:', alpha=0.75)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG8VMCq77bF-"
      },
      "outputs": [],
      "source": [
        "#@title Display Results. { vertical-output: true }\n",
        "\n",
        "samples_per_page = 25  #@param\n",
        "page_state = display.PageState(\n",
        "    np.ceil(len(combined_results) / samples_per_page))\n",
        "\n",
        "display.display_paged_results(\n",
        "    search.TopKSearchResults(len(combined_results), combined_results),\n",
        "    page_state, samples_per_page,\n",
        "    project_state=project_state,\n",
        "    embedding_sample_rate=project_state.embedding_model.sample_rate,\n",
        "    exclusive_labels=True,\n",
        "    checkbox_labels=[target_class, f'not {target_class}', 'unsure'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPrJlBgU7cTc"
      },
      "outputs": [],
      "source": [
        "#@title Collate results and write validation log. { vertical-output: true }\n",
        "\n",
        "filenames = []\n",
        "timestamp_offsets = []\n",
        "scores = []\n",
        "is_pos = []\n",
        "weights = []\n",
        "bins = []\n",
        "\n",
        "# Compute the sampling weights for each bin.\n",
        "bin_wts = [1.0 / 2**(k + 1) for k in range(num_bins - 1)]\n",
        "bin_wts.append(bin_wts[-1])\n",
        "\n",
        "for r in combined_results:\n",
        "  if not r.label_widgets: continue\n",
        "  value = r.label_widgets[0].value\n",
        "  if value is None:\n",
        "    continue\n",
        "  filenames.append(r.filename)\n",
        "  scores.append(r.score)\n",
        "  timestamp_offsets.append(r.timestamp_offset)\n",
        "\n",
        "  # Get the bin number and sampling weight for the search result.\n",
        "  result_bin = np.argmax(r.score < q_bounds) - 1\n",
        "  bins.append(result_bin)\n",
        "  weights.append(bin_wts[result_bin])\n",
        "\n",
        "  if value == target_class:\n",
        "    is_pos.append(1)\n",
        "  elif value == f'not {target_class}':\n",
        "    is_pos.append(-1)\n",
        "  elif value == 'unsure':\n",
        "    is_pos.append(0)\n",
        "\n",
        "label = [target_class for _ in range(len(filenames))]\n",
        "log = pd.DataFrame({\n",
        "    'filenames': filenames,\n",
        "    'timestamp_offsets': timestamp_offsets,\n",
        "    'scores': scores,\n",
        "    'is_pos': is_pos,\n",
        "    'weights': weights,\n",
        "    'bins': bins,\n",
        "})\n",
        "log.to_csv(validation_log_filepath, mode='a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmtqKln1vEAu"
      },
      "outputs": [],
      "source": [
        "#@title delete\n",
        "bounds = [0.0, 0.9, 0.99, 0.999, 1.0]  #@param\n",
        "#@markdown Number of validation samples per bin.\n",
        "samples_per_bin = 25  #@param\n",
        "\n",
        "bounds = np.array(bounds)\n",
        "num_bins = len(bounds) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNutTJ397dfv"
      },
      "outputs": [],
      "source": [
        "#@title Estimate Call Density. { vertical-output: true }\n",
        "\n",
        "import scipy\n",
        "\n",
        "# Collect validated labels by bin.\n",
        "bin_pos = [0 for i in range(num_bins)]\n",
        "bin_neg = [0 for i in range(num_bins)]\n",
        "for score, pos in zip(scores, is_pos):\n",
        "  result_bin = np.argmax(score < q_bounds) - 1\n",
        "  if pos == 1:\n",
        "    bin_pos[result_bin] += 1\n",
        "  elif pos == -1:\n",
        "    bin_neg[result_bin] += 1\n",
        "\n",
        "# Create beta distributions.\n",
        "prior = 0.1\n",
        "betas = [scipy.stats.beta(p + prior, n + prior)\n",
        "         for p, n in zip(bin_pos, bin_neg)]\n",
        "# MLE positive rate in each bin.\n",
        "mle_b = np.array([bin_pos[b] / (bin_pos[b] + bin_neg[b] + 1e-6)\n",
        "                  for b in range(num_bins)])\n",
        "# Probability of each bin, P(b).\n",
        "p_b = np.array([2**-k for k in range(1, num_bins)] + [2**(-num_bins + 1)])\n",
        "\n",
        "# MLE total call density.\n",
        "q_mle = np.dot(mle_b, p_b)\n",
        "\n",
        "num_beta_samples = 10_000\n",
        "q_betas = []\n",
        "for _ in range(num_beta_samples):\n",
        "  qs_pos = np.array([b.rvs(size=1)[0] for b in betas])  # P(+|b)\n",
        "  q_beta = np.dot(qs_pos, p_b)\n",
        "  q_betas.append(q_beta)\n",
        "\n",
        "# Plot call density estimate.\n",
        "plt.figure(figsize=(10, 5))\n",
        "xs, ys, _ = plt.hist(q_betas, density=True, bins=25, alpha=0.25)\n",
        "plt.plot([q_mle, q_mle], [0.0, np.max(xs)], 'k:', alpha=0.75,\n",
        "         label='q_mle')\n",
        "\n",
        "low, high = np.quantile(q_betas, [0.05, 0.95])\n",
        "plt.plot([low, low], [0.0, np.max(xs)], 'g', alpha=0.75, label='low conf')\n",
        "plt.plot([high, high], [0.0, np.max(xs)], 'g', alpha=0.75, label='high conf')\n",
        "\n",
        "plt.xlim(0.0, 1.0)\n",
        "plt.xlabel('Call Rate (q)')\n",
        "plt.ylabel('P(q)')\n",
        "plt.title(f'Call Density Estimation ({target_class})')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f'MLE Call Density: {q_mle:.4f}')\n",
        "print(f'(Low/MLE/High) Call Density Estimate: ({low:5.4f} / {q_mle:5.4f} / {high:5.4f})')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uYtFyXz7fXW"
      },
      "outputs": [],
      "source": [
        "#@title Naive Estimation of ROC-AUC for target class. { vertical-output: true }\n",
        "#@markdown Computes ROC-AUC from the validation logs, with bin weighting.\n",
        "#@markdown ROC-AUC is the overall probability that a random positive example\n",
        "#@markdown has a higher classifier score than a random negative example.\n",
        "\n",
        "# Probability of bins\n",
        "p_b = [1.0 / 2**(k + 1) for k in range(num_bins - 1)]\n",
        "p_b.append(p_b[-1])\n",
        "p_b = np.array(p_b)\n",
        "\n",
        "bin_pos, bin_neg = np.array(bin_pos), np.array(bin_neg)\n",
        "# Compute P(z(+) > z(-))\n",
        "# P(b_i|+) * P(b_k|-)\n",
        "n_pos = np.sum(bin_pos)\n",
        "n_neg = np.sum(bin_neg)\n",
        "p_pos_b = np.array(bin_pos) / (bin_pos + bin_neg)\n",
        "p_neg_b = np.array(bin_neg) / (bin_pos + bin_neg)\n",
        "p_pos = np.sum(p_pos_b * p_b)\n",
        "p_neg = np.sum(p_neg_b * p_b)\n",
        "\n",
        "p_b_pos = p_pos_b * p_b / p_pos\n",
        "p_b_neg = p_neg_b * p_b / p_neg\n",
        "roc_auc = 0\n",
        "# For off-diagonal bin pairs:\n",
        "# Take the probability of drawing a pos from bin j and neg from bin i.\n",
        "# If j > i, all pos examples are scored higher, so contributes directly to the\n",
        "# total ROC-AUC.\n",
        "for i in range(num_bins):\n",
        "  for j in range(i + 1, num_bins):\n",
        "    roc_auc += p_b_pos[j] * p_b_neg[i]\n",
        "\n",
        "# For diagonal bin-pairs:\n",
        "# Look at actual in-bin observations for diagonal contribution.\n",
        "bins = np.array(bins)\n",
        "is_pos = np.array(is_pos)\n",
        "\n",
        "for b in range(num_bins):\n",
        "  bin_pos_idxes = np.argwhere((bins == b) * (is_pos == 1))[:, 0]\n",
        "  bin_neg_idxes = np.argwhere((bins == b) * (is_pos == -1))[:, 0]\n",
        "  bin_pos_scores = np.array(scores)[bin_pos_idxes]\n",
        "  bin_neg_scores = np.array(scores)[bin_neg_idxes]\n",
        "  if bin_pos_scores.size == 0:\n",
        "    continue\n",
        "  if bin_neg_scores.size == 0:\n",
        "    continue\n",
        "  # Count total number of pairs where the pos examples have a higher score than\n",
        "  # a negative example.\n",
        "  hits = ((bin_pos_scores[:, np.newaxis]\n",
        "           - bin_neg_scores[np.newaxis, :]) > 0).sum()\n",
        "  bin_roc_auc = hits / (bin_pos_scores.size * bin_neg_scores.size)\n",
        "  # Contribution is the probability of pulling both pos and neg examples\n",
        "  # from this bin, multiplied by the bin's ROC-AUC.\n",
        "  roc_auc += bin_roc_auc * p_b_pos[b] * p_b_neg[b]\n",
        "\n",
        "print(f'ROC-AUC : {roc_auc:.3f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
